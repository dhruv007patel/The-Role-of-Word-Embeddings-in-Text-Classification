{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsPdUDXSMQUF"
      },
      "outputs": [],
      "source": [
        "# required model to pretrain for fasttext \n",
        "#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "# !gunzip cc.en.300.bin.gz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V8oRny-C-sE"
      },
      "source": [
        "Importing all libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0q4dn-NzeCu",
        "outputId": "a143cacf-9dc7-4d35-b11e-f992f5a3344b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import json\n",
        "import os, sys, optparse, gzip, re, logging, string\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch \n",
        "from sklearn.utils import shuffle\n",
        "from torch.optim import Adam\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import fasttext\n",
        "import numpy as np\n",
        "import nltk\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaHTMK8ADNWr"
      },
      "source": [
        "Loading Dataset: The preprocessed data is used for here.  \n",
        "Loading pre-trained fasttext embeddings for reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veJhwhqx8sK1",
        "outputId": "a53310a7-0591-4a97-8063-ce289ef0bd6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "processed_df = pd.read_csv(\"../data/processed_df.csv\")\n",
        "\n",
        "# Load pre-trained FastText embeddings\n",
        "fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNfJ2W-fDUX4"
      },
      "source": [
        "The text_to_embedding function takes a string of text and a maximum length, and returns a numpy array of word embeddings for the text.\n",
        "\n",
        "The function first tokenizes the text into individual words using the word_tokenize function from the nltk package. It then truncates the list of tokens to the maximum length specified.\n",
        "\n",
        "Next, the function uses a pre-trained FastText model to obtain the word embeddings for each token in the list. If a token is not present in the pre-trained model's vocabulary, the function adds a zero vector to the list of embeddings.\n",
        "\n",
        "If the list of embeddings is shorter than the maximum length, the function pads the list with additional zero vectors to match the maximum length.\n",
        "\n",
        "Finally, the function returns the list of embeddings as a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XJME6E0s86z2"
      },
      "outputs": [],
      "source": [
        "def text_to_embedding(text, max_length):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = tokens[:max_length]  # Truncate to max_length\n",
        "    embeddings = [fasttext_model.get_word_vector(token) for token in tokens]\n",
        "    padding_length = max_length - len(embeddings)\n",
        "    embeddings.extend([np.zeros(fasttext_model.get_dimension()) for _ in range(padding_length)])\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUDW3olrDY0Z"
      },
      "source": [
        "Fasttext + CNN implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "17FSS9Xh9Ayp"
      },
      "outputs": [],
      "source": [
        "class FastTextCNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_filters, filter_sizes, num_classes, dropout_prob):\n",
        "        super(FastTextCNN, self).__init__()\n",
        "\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(fs, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=num_filters, out_channels=num_filters, kernel_size=(fs, 1))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add channel dimension (batch_size, 1, max_length, embedding_dim)\n",
        "\n",
        "        # Apply first set of convolutional layers with different filter sizes\n",
        "        conv_outputs1 = []\n",
        "        for conv1 in self.convs1:\n",
        "            conv_output1 = torch.relu(conv1(x))\n",
        "            conv_outputs1.append(conv_output1)\n",
        "\n",
        "        # Apply second set of convolutional layers with different filter sizes\n",
        "        conv_outputs2 = []\n",
        "        for conv2, conv_output1 in zip(self.convs2, conv_outputs1):\n",
        "            conv_output2 = torch.relu(conv2(conv_output1)).squeeze(3)\n",
        "            pooled_output = torch.max_pool1d(conv_output2, conv_output2.size(2)).squeeze(2)\n",
        "            conv_outputs2.append(pooled_output)\n",
        "\n",
        "        x = torch.cat(conv_outputs2, 1)\n",
        "        x = self.dropout(x)  # Dropout layer\n",
        "        x = self.fc(x)  # Fully connected layer (batch_size, num_classes)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E21w_xf9DbpJ"
      },
      "source": [
        "Fasttext + LSTM implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MjIfwgZw96Gu"
      },
      "outputs": [],
      "source": [
        "class FastTextBiLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_classes, dropout):\n",
        "        super(FastTextBiLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "        out, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Concatenate the hidden states of the forward and backward LSTM layers\n",
        "        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "d7YcggJ29GNw"
      },
      "outputs": [],
      "source": [
        "def data_split(processed_df, train=.75, test=.15):\n",
        "    np.random.seed(111)\n",
        "    df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=40), \n",
        "                                     [int(train*len(df)), int((1-test)*len(df))])\n",
        "    \n",
        "    return df_train, df_val, df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN9xC9iUDhsZ"
      },
      "source": [
        "Training and test functions for model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q7Hnwh7Z9J7T"
      },
      "outputs": [],
      "source": [
        "def training(model, train_loader, val_loader, criterion, optimizer, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), num_epochs=20, modeltype='CNN'):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop-\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                val_predictions = torch.argmax(outputs, dim=1)\n",
        "                val_accuracy += (val_predictions == labels).float().mean().item()\n",
        "\n",
        "        # Print epoch results\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_loader)\n",
        "        # Save the best model based on validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict()\n",
        "            torch.save(best_model, '../data/fasttext/fasttext_best_model_{}.pth'.format(modeltype))\n",
        "                \n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pj7OH3VV9Om4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            test_predictions = torch.argmax(outputs, dim=1)\n",
        "            test_accuracy += (test_predictions == labels).float().mean().item()\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(test_predictions.cpu().numpy())\n",
        "\n",
        "    test_accuracy /= len(test_loader)\n",
        "    \n",
        "    # Convert label indices to label names\n",
        "    classes = le.inverse_transform(list(range(num_classes)))\n",
        "    true_labels_name = [classes[label_index] for label_index in true_labels]\n",
        "    predicted_labels_name = [classes[label_index] for label_index in predicted_labels]\n",
        "    precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
        "    \n",
        "    print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}')\n",
        "\n",
        "\n",
        "    return true_labels_name, predicted_labels_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4aGH_dE09Szt"
      },
      "outputs": [],
      "source": [
        "df=processed_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB7E6DyYDljM"
      },
      "source": [
        "Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlO1SN9M9W8f",
        "outputId": "75815bf7-b5b5-41e8-9bf1-aa0c69750187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df_train, df_val, df_test = data_split(df)\n",
        "train_texts = df_train['text'].astype(str).tolist()\n",
        "\n",
        "train_labels = df_train['category'].tolist()\n",
        "val_texts = df_val['text'].astype(str).tolist()\n",
        "val_labels = df_val['category'].tolist()\n",
        "test_texts = df_test['text'].astype(str).tolist()\n",
        "test_labels = df_test['category'].tolist()\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['category'] = le.fit_transform(df['category'])\n",
        "texts = df['text'].tolist()\n",
        "labels = df['category'].tolist()\n",
        "embedding_dim = fasttext_model.get_dimension()  \n",
        "num_filters = 200\n",
        "filter_sizes = [2, 3, 4]\n",
        "num_classes = len(le.classes_)\n",
        "print(num_classes)\n",
        "max_length = 250\n",
        "hidden_dim= 64\n",
        "num_layers=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMep46Q8DsB0"
      },
      "source": [
        "Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8J_09Vd69aHV"
      },
      "outputs": [],
      "source": [
        "class DatasetLoader(Dataset):\n",
        "    def __init__(self, texts, labels, text_to_embedding, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.text_to_embedding = text_to_embedding\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        text_embedding = self.text_to_embedding(text, self.max_length)\n",
        "        return torch.tensor(text_embedding, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pw8X5ZxV9cH3"
      },
      "outputs": [],
      "source": [
        "# Create a mapping of labels to integers\n",
        "label_to_idx = {label: idx for idx, label in enumerate(set(train_labels))}\n",
        "train_labels_num = [label_to_idx[label] for label in train_labels]\n",
        "val_labels_num = [label_to_idx[label] for label in val_labels]\n",
        "test_labels_num = [label_to_idx[label] for label in test_labels]\n",
        "\n",
        "train_dataset = DatasetLoader(train_texts, train_labels_num, text_to_embedding, max_length)\n",
        "val_dataset = DatasetLoader(val_texts, val_labels_num, text_to_embedding, max_length)\n",
        "test_dataset = DatasetLoader(test_texts, test_labels_num, text_to_embedding, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YXLMFEQ59niM"
      },
      "outputs": [],
      "source": [
        "model_cnn = FastTextCNN(embedding_dim, num_filters, filter_sizes, num_classes, dropout_prob=0.2).to(device)\n",
        "learning_rate = 0.001  \n",
        "optimizer = optim.Adam(model_cnn.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wf-awr99tDY"
      },
      "source": [
        "FASTTEXT+CNN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f31VXYO9prU",
        "outputId": "c15421d1-7f29-4888-c1f3-fc8dff73f7f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Training Loss: 1.2115, Validation Loss: 1.0172, Validation Accuracy: 68.70%\n",
            "Epoch 2/5, Training Loss: 0.9819, Validation Loss: 0.9686, Validation Accuracy: 69.68%\n",
            "Epoch 3/5, Training Loss: 0.8962, Validation Loss: 0.9737, Validation Accuracy: 69.64%\n",
            "Epoch 4/5, Training Loss: 0.8181, Validation Loss: 0.9593, Validation Accuracy: 70.42%\n",
            "Epoch 5/5, Training Loss: 0.7392, Validation Loss: 0.9818, Validation Accuracy: 70.42%\n"
          ]
        }
      ],
      "source": [
        "training(model_cnn, train_loader, val_loader, criterion, optimizer, device=device, num_epochs=5, modeltype='CNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M46Dd6tn9xg3"
      },
      "source": [
        "FASTTEXT+CNN Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW4DjN7B9rlk",
        "outputId": "56a87417-0d03-45ef-a781-f6ae65380c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 70.30%\n",
            "Precision: 0.70, Recall: 0.70, F1-score: 0.69\n"
          ]
        }
      ],
      "source": [
        "true_labels_name_CNN, predicted_labels_name_CNN = test(model_cnn, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-H-Nhkon99mM"
      },
      "outputs": [],
      "source": [
        "model_lstm = FastTextBiLSTM(embedding_dim, hidden_dim, num_layers, num_classes, dropout=0.2).to(device)\n",
        "learning_rate = 0.001  \n",
        "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKcf9bv2Dxq7"
      },
      "source": [
        "FASTTEXT+LSTM Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc2Lu9mA-poS",
        "outputId": "26f79f80-9ada-4069-af6e-316d7c383643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Training Loss: 1.4100, Validation Loss: 1.1280, Validation Accuracy: 65.54%\n",
            "Epoch 2/5, Training Loss: 1.0595, Validation Loss: 0.9883, Validation Accuracy: 69.30%\n",
            "Epoch 3/5, Training Loss: 0.9568, Validation Loss: 0.9229, Validation Accuracy: 71.27%\n",
            "Epoch 4/5, Training Loss: 0.8851, Validation Loss: 0.8889, Validation Accuracy: 71.95%\n",
            "Epoch 5/5, Training Loss: 0.8337, Validation Loss: 0.8670, Validation Accuracy: 72.59%\n"
          ]
        }
      ],
      "source": [
        "training(model_lstm, train_loader, val_loader, criterion, optimizer_lstm, device=device, num_epochs=5, modeltype='LSTM')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdKf-ymYD3Fp"
      },
      "source": [
        "FASTTEXT+LSTM Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TzDtQBY-rtA",
        "outputId": "04d10a4d-19a6-4dc2-a0ff-43586ccec719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 72.45%\n",
            "Precision: 0.72, Recall: 0.72, F1-score: 0.72\n"
          ]
        }
      ],
      "source": [
        "true_labels_name_lstm, predicted_labels_name_lstm = test(model_lstm, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-jc-JdGD7JY"
      },
      "source": [
        "To run the best model for fasttext + CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk55g1knztnq",
        "outputId": "e7b9bfe4-1fd8-48a3-9899-d2aa3b451253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 70.66%\n",
            "Precision: 0.71, Recall: 0.71, F1-score: 0.70\n"
          ]
        }
      ],
      "source": [
        "saved_model_path = \"../data/fasttext/fasttext_best_model_CNN.pth\"\n",
        "saved_model_state_dict = torch.load(saved_model_path)\n",
        "num_filters = 200\n",
        "filter_sizes = [2, 3, 4]\n",
        "num_classes = 15\n",
        "max_length = 250\n",
        "\n",
        "# Model initialization\n",
        "model = FastTextCNN(embedding_dim, num_filters, filter_sizes, num_classes, dropout_prob=0.2).to(device)\n",
        "\n",
        "# Load the saved state dictionary into your model\n",
        "model.load_state_dict(saved_model_state_dict)\n",
        "true_labels_name_cnn, predicted_labels_name_cnn=test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIK0RZOVD_uu"
      },
      "source": [
        "To run the best model for fasttext + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmHnAVqU0cA9",
        "outputId": "e409af95-c260-4e2b-c038-af7b24f19b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 72.45%\n",
            "Precision: 0.72, Recall: 0.72, F1-score: 0.72\n"
          ]
        }
      ],
      "source": [
        "saved_model_path = \"/content/drive/MyDrive/data/fasttext/fasttext_best_model_LSTM.pth\"\n",
        "saved_model_state_dict = torch.load(saved_model_path)\n",
        "num_filters = 200\n",
        "filter_sizes = [2, 3, 4]\n",
        "num_classes = 15\n",
        "max_length = 250\n",
        "hidden_dim= 64\n",
        "num_layers=3\n",
        "\n",
        "# Model initialization\n",
        "model = FastTextBiLSTM(embedding_dim, hidden_dim, num_layers, num_classes, dropout=0.2).to(device)\n",
        "# Load the saved state dictionary into your model\n",
        "model.load_state_dict(saved_model_state_dict)\n",
        "true_labels_name_lstm, predicted_labels_name_lstm=test(model, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
