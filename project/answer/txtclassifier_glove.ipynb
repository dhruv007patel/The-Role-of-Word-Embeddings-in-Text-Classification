{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc6af17",
   "metadata": {},
   "source": [
    "## Text Classification on News Articles (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ef547",
   "metadata": {},
   "source": [
    "The following notebook explains the methods and model used to develop the final version of the text classifier built based off of GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44897691",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5172d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from txtclassifier_glove import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1eee0",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b310597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a saved version of pre-processed file. \n",
    "df = pd.read_csv(\"../data/processed_df.csv\", header=0).drop('Unnamed: 0', axis=1)\n",
    "X = df['text']\n",
    "y = df['category']\n",
    "num_classes = len(y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb693f3",
   "metadata": {},
   "source": [
    "### Loading glove word embeddings\n",
    "\n",
    "The following code-snippet loads a pre-trained GloVe word embedding file, reads its contents, and creates a dictionary with each word as a key and its corresponding 100-dimensional vector as a value. The resulting `embedding_dict` can be used to look up the vector representation of any word in the GloVe embeddings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025c1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding_dict = {}\n",
    "with open('./data/glove/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_dict[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0ebe6",
   "metadata": {},
   "source": [
    "### Creating word to index mappings\n",
    "\n",
    "The following code preprocesses text data, creates a word-to-index mapping for each unique word in the texts, and calculates the vocabulary size by counting the number of unique words in the **word-to-index** mapping. The resulting word_to_idx dictionary can be used to look up the index of each word in the vocabulary. The vocab_size variable holds the total number of unique words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed02373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word-to-index mapping\n",
    "max_seq_len = 100\n",
    "word_to_idx = {}\n",
    "idx = 1\n",
    "for text in X:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word not in word_to_idx and word in embedding_dict:\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "            \n",
    "vocab_size = len(word_to_idx) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa9acd",
   "metadata": {},
   "source": [
    "### Splitting the data and loading the data\n",
    "\n",
    "The below code converts the text data into sequences of word indices and one-hot encoded labels for the training, validation, and test sets. The resulting sequences of indices are truncated or padded with zeros to match the max_seq_len variable. The data is then converted to PyTorch tensors of appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab88762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = data_split(df)\n",
    "train_texts = df_train['text'].astype(str).tolist()\n",
    "train_labels = df_train['category'].tolist()\n",
    "val_texts = df_val['text'].astype(str).tolist()\n",
    "val_labels = df_val['category'].tolist()\n",
    "test_texts = df_test['text'].astype(str).tolist()\n",
    "test_labels = df_test['category'].tolist()\n",
    "\n",
    "train_texts = [[word_to_idx[word] for word in text.split() if word in word_to_idx][:max_seq_len] for text in train_texts]\n",
    "train_texts = torch.tensor([xi + [0]*(max_seq_len - len(xi)) for xi in train_texts], dtype=torch.long)\n",
    "train_labels = torch.tensor(pd.get_dummies(train_labels).values, dtype=torch.float32)\n",
    "\n",
    "val_texts = [[word_to_idx[word] for word in text.split() if word in word_to_idx][:max_seq_len] for text in val_texts]\n",
    "val_texts = torch.tensor([xi + [0]*(max_seq_len - len(xi)) for xi in val_texts], dtype=torch.long)\n",
    "val_labels = torch.tensor(pd.get_dummies(val_labels).values, dtype=torch.float32)\n",
    "\n",
    "test_texts = [[word_to_idx[word] for word in text.split() if word in word_to_idx][:max_seq_len] for text in test_texts]\n",
    "test_texts = torch.tensor([xi + [0]*(max_seq_len - len(xi)) for xi in test_texts], dtype=torch.long)\n",
    "test_labels = torch.tensor(pd.get_dummies(test_labels).values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74269fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_utils.TensorDataset(train_texts, train_labels)\n",
    "val_dataset = data_utils.TensorDataset(val_texts, val_labels)\n",
    "test_dataset = data_utils.TensorDataset(test_texts, test_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data_utils.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee77c9",
   "metadata": {},
   "source": [
    "###  Building different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bbb60",
   "metadata": {},
   "source": [
    "### Simple CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c57424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embedding_dict, num_classes, max_seq_len):\n",
    "        super(TextCNN, self).__init__()\n",
    "        embedding_dim = 100\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(np.random.normal(0, 1, (vocab_size, embedding_dim))))\n",
    "        for word, idx in word_to_idx.items():\n",
    "            if word in embedding_dict:\n",
    "                self.embedding.weight.data[idx] = torch.from_numpy(embedding_dict[word])\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3)\n",
    "        self.maxpool = nn.MaxPool1d(max_seq_len - 3 + 1)\n",
    "        self.fc = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(-1, 100)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21280d4",
   "metadata": {},
   "source": [
    "### Enhanced CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6661b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN_enhanced(nn.Module):\n",
    "    def __init__(self, embedding_dict, num_classes, max_seq_len):\n",
    "        super(TextCNN_enhanced, self).__init__()\n",
    "        embedding_dim = 100\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(np.random.normal(0, 1, (vocab_size, embedding_dim))))\n",
    "        for word, idx in word_to_idx.items():\n",
    "            if word in embedding_dict:\n",
    "                self.embedding.weight.data[idx] = torch.from_numpy(embedding_dict[word])\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=5)\n",
    "        self.maxpool = nn.MaxPool1d(max_seq_len - 3 - 4 - 5 + 3 + 1 + 1)\n",
    "        self.fc = nn.Linear(300, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x1 = nn.functional.relu(self.conv1(x))\n",
    "        x2 = nn.functional.relu(self.conv2(x))\n",
    "        x3 = nn.functional.relu(self.conv3(x))\n",
    "        x1 = self.maxpool(x1)\n",
    "        x2 = self.maxpool(x2)\n",
    "        x3 = self.maxpool(x3)\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = x.view(-1, 300)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40f065",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8a65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, save_every_n_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_model = None\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = model\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        if epoch % save_every_n_epochs == 0:\n",
    "            # Save the model\n",
    "            if type(model).__name__ == 'TextCNN_enhanced':\n",
    "                best_model_path = f'../data/glove/glove_cnn_enhance_{epoch}.pth'\n",
    "            elif type(model).__name__ == 'TextCNN':\n",
    "                best_model_path = f'../data/glove/glove_cnn_{epoch}.pth'\n",
    "            if best_model is not None:\n",
    "                torch.save(best_model.state_dict(), best_model_path)\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}')\n",
    "\n",
    "    return best_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c866b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN model\n",
      "Epoch 0: train loss = 1.1170, val loss = 0.8215\n",
      "Epoch 1: train loss = 0.8227, val loss = 0.6468\n",
      "Epoch 2: train loss = 0.6557, val loss = 0.4648\n",
      "Epoch 3: train loss = 0.4999, val loss = 0.3177\n",
      "Epoch 4: train loss = 0.3579, val loss = 0.2135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TextCNN(\n",
       "   (embedding): Embedding(59959, 100)\n",
       "   (conv1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
       "   (maxpool): MaxPool1d(kernel_size=98, stride=98, padding=0, dilation=1, ceil_mode=False)\n",
       "   (fc): Linear(in_features=100, out_features=15, bias=True)\n",
       " ),\n",
       " [1.1169984333701108,\n",
       "  0.8227214720292825,\n",
       "  0.655671860916362,\n",
       "  0.4999406697196955,\n",
       "  0.357872627222395],\n",
       " [0.821484368431652,\n",
       "  0.6467710630609864,\n",
       "  0.4647854510671623,\n",
       "  0.31771604419155314,\n",
       "  0.21345315998235936])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "cnn_model = TextCNN(embedding_dict, num_classes = num_classes, max_seq_len = max_seq_len)\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Takes about 30 mins for a complete run\n",
    "print(\"Training CNN model\")\n",
    "train(cnn_model, optimizer, nn.CrossEntropyLoss(), train_loader, val_loader, num_epochs,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0015f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Enhanced TextCNN model\n",
      "Epoch 0: train loss = 1.0784, val loss = 0.7910\n",
      "Epoch 1: train loss = 0.7841, val loss = 0.5609\n",
      "Epoch 2: train loss = 0.5899, val loss = 0.3664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TextCNN_enhanced(\n",
       "   (embedding): Embedding(59959, 100)\n",
       "   (conv1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
       "   (conv2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
       "   (conv3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
       "   (maxpool): MaxPool1d(kernel_size=93, stride=93, padding=0, dilation=1, ceil_mode=False)\n",
       "   (fc): Linear(in_features=300, out_features=15, bias=True)\n",
       " ),\n",
       " [1.0783945596940665, 0.7840815012150182, 0.5899295413764144],\n",
       " [0.7910271767476003, 0.5608900474414482, 0.36639281172254484])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "enhanced_cnn_model = TextCNN_enhanced(embedding_dict, num_classes = num_classes, max_seq_len = max_seq_len)\n",
    "optimizer = torch.optim.Adam(enhanced_cnn_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Takes about 30-45 mins for a complete run\n",
    "print(\"Training Enhanced TextCNN model\")\n",
    "train(enhanced_cnn_model, optimizer, nn.CrossEntropyLoss(), train_loader, val_loader, num_epochs,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a3833",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45db6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_model, test_loader):\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    test_model.eval()\n",
    "\n",
    "    # Calculate the accuracy on the test set\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        predicted_labels = []\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = test_model(X_batch)\n",
    "            y_pred = y_pred.argmax(dim=1)\n",
    "            predicted_labels.extend(y_pred.tolist())\n",
    "\n",
    "        true_labels = []\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            true_labels.extend(y_batch.tolist())\n",
    "\n",
    "        true_labels = [torch.argmax(torch.tensor(batch_labels)) for batch_labels in true_labels]\n",
    "        true_labels = torch.tensor(true_labels, dtype=torch.int64)\n",
    "        predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "        class_counts = torch.bincount(true_labels)\n",
    "        correct_counts = torch.bincount(true_labels[predicted_labels == true_labels], minlength=len(class_counts))\n",
    "\n",
    "        accuracy = float(correct_counts.sum()) / float(class_counts.sum())\n",
    "        precision = float(correct_counts[1]) / float(class_counts[1])\n",
    "        recall = float(correct_counts[1]) / float(class_counts[1] + class_counts[0])\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eceb583",
   "metadata": {},
   "source": [
    "### Evaluation Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b1597",
   "metadata": {},
   "source": [
    "#### Simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a67172b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Accuracy: 0.6756\n",
      "Precision: 0.5743\n",
      "Recall: 0.4479\n",
      "F1 Score: 0.5033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model = TextCNN(embedding_dict, num_classes = num_classes, max_seq_len = max_seq_len)\n",
    "\n",
    "# Load the saved model state dict\n",
    "state_dict = torch.load('../data/glove/glove_cnn_4.pth')\n",
    "\n",
    "# Load the state dict into the model\n",
    "test_model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "test_model.eval()\n",
    "\n",
    "accuracy, precision, recall, f1_score = evaluate_model(test_model, test_loader)\n",
    "\n",
    "print(\"Evaluation Results:\\n\"\n",
    "      \"Accuracy: {:.4f}\\n\"\n",
    "      \"Precision: {:.4f}\\n\"\n",
    "      \"Recall: {:.4f}\\n\"\n",
    "      \"F1 Score: {:.4f}\\n\".format(accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa2c5d",
   "metadata": {},
   "source": [
    "#### Enhanced CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9017bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Accuracy: 0.7006\n",
      "Precision: 0.5970\n",
      "Recall: 0.4656\n",
      "F1 Score: 0.5232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model = TextCNN_enhanced(embedding_dict, num_classes = num_classes, max_seq_len = max_seq_len)\n",
    "\n",
    "# Load the saved model state dict\n",
    "state_dict = torch.load('../data/glove/glove_cnn_enhance_2.pth')\n",
    "\n",
    "# Load the state dict into the model\n",
    "test_model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "test_model.eval()\n",
    "\n",
    "accuracy, precision, recall, f1_score = evaluate_model(test_model, test_loader)\n",
    "\n",
    "print(\"Evaluation Results:\\n\"\n",
    "      \"Accuracy: {:.4f}\\n\"\n",
    "      \"Precision: {:.4f}\\n\"\n",
    "      \"Recall: {:.4f}\\n\"\n",
    "      \"F1 Score: {:.4f}\\n\".format(accuracy, precision, recall, f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
